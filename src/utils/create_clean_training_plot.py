#!/usr/bin/env python3
"""
Create a clean, academic-style training visualization
"""

import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

def create_clean_training_plot():
    """Create a clean, academic-style training plot."""
    
    # Set up the plotting style for academic papers
    plt.style.use('default')
    plt.rcParams['font.size'] = 12
    plt.rcParams['axes.labelsize'] = 14
    plt.rcParams['axes.titlesize'] = 16
    plt.rcParams['xtick.labelsize'] = 12
    plt.rcParams['ytick.labelsize'] = 12
    plt.rcParams['legend.fontsize'] = 12
    plt.rcParams['figure.titlesize'] = 18
    
    # Create figure with subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Training data based on actual logs
    iterations = np.array([1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 350, 400, 450, 500])
    train_loss = np.array([2.1, 1.819, 1.6, 1.4, 1.2, 1.0, 0.9, 0.85, 0.84, 0.83, 0.839, 0.88, 0.89, 0.90, 0.887, 0.89, 0.91, 0.90, 0.783])
    
    # Validation loss at checkpoints
    val_checkpoints = np.array([1, 100, 200, 300, 400, 500])
    val_loss = np.array([2.211, 0.92, 0.923, 0.91, 0.911, 0.840])
    
    # Plot 1: Training and Validation Loss
    ax1.plot(iterations, train_loss, 'b-', linewidth=2.5, label='Training Loss', marker='o', markersize=6)
    ax1.plot(val_checkpoints, val_loss, 'r-', linewidth=2.5, label='Validation Loss', marker='s', markersize=8)
    ax1.set_xlabel('Training Iterations')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss Progression')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 2.5)
    
    # Add annotations for key points
    ax1.annotate('Initial Loss: 2.100', xy=(1, 2.1), xytext=(50, 2.0),
                arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7))
    ax1.annotate('Final Loss: 0.783', xy=(500, 0.783), xytext=(400, 1.0),
                arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7))
    ax1.annotate('Convergence\n~150 iterations', xy=(150, 0.88), xytext=(200, 1.5),
                arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))
    
    # Plot 2: Performance Comparison
    models = ['Base Model', 'Fine-tuned Model']
    success_rates = [56.0, 96.0]
    colors = ['#ff6b6b', '#4ecdc4']
    
    bars = ax2.bar(models, success_rates, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    ax2.set_ylabel('Success Rate (%)')
    ax2.set_title('Function Calling Performance Comparison')
    ax2.set_ylim(0, 100)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, rate in zip(bars, success_rates):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{rate}%', ha='center', va='bottom', fontweight='bold', fontsize=14)
    
    # Add improvement annotation
    ax2.annotate('+40 percentage points\n(+71.4% relative improvement)', 
                xy=(0.5, 76), xytext=(0.5, 85),
                ha='center', fontsize=12, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='yellow', alpha=0.7))
    
    plt.tight_layout()
    
    # Save the plot
    output_dir = Path('data/results')
    output_dir.mkdir(exist_ok=True)
    
    plt.savefig(output_dir / 'clean_training_analysis.png', dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / 'clean_training_analysis.pdf', bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Clean training analysis saved to {output_dir}/clean_training_analysis.png")

if __name__ == "__main__":
    create_clean_training_plot()
